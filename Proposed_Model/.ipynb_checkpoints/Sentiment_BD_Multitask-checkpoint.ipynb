{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bkemx_k019DQ",
    "outputId": "b26b166a-f1b5-4f83-b36c-4c73dd8206c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Loading drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9khuUr_2Isb",
    "outputId": "32cb56b5-9f8a-4b03-8e4c-6a6a5bf30f09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrkzQELU2MRx"
   },
   "outputs": [],
   "source": [
    "# All general imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer \n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Bidirectional, GlobalAveragePooling1D, GRU, GlobalMaxPooling1D, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LSTM, GRU, Conv1D, MaxPool1D, Activation, Add\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import SpatialDropout1D\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, Conv1D, Softmax\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import io, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeof6FtE2P43",
    "outputId": "d493b4d1-80f9-46e7-cbd7-7e7aec06af87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive  sample_data\n",
      "/content/drive/My Drive/Fake_News_Data\n",
      "/content/drive/My Drive/Fake_News_Data\n"
     ]
    }
   ],
   "source": [
    "# Setting the working directory \n",
    "!ls\n",
    "%cd drive/My\\ Drive/Fake_News_Data\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 846
    },
    "id": "NewHPpcP2TCI",
    "outputId": "54e37798-2e31-46e6-a8b8-b2b26623528e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'title1_en', 'title2_en', 'bd_label', 'quora_label',\n",
      "       'Premise_Emotion_Label', 'Hypothesis_Emotion_Label', 'femotion',\n",
      "       'NV_Orc', 'EM_Orc', 'weights', 'sentiment_scores_pre',\n",
      "       'sentiment_scores_hyp', 'sentiment_pre_labels', 'sentiment_hyp_labels',\n",
      "       'Combined_Sentiment', 'com_femotion'],\n",
      "      dtype='object')\n",
      "Index(['id', 'title1_en', 'title2_en', 'bd_label', 'quora_label',\n",
      "       'Premise_Emotion_Label', 'Hypothesis_Emotion_Label', 'femotion',\n",
      "       'NV_Orc', 'EM_Orc', 'weights', 'test_wts', 'sentiment_scores_pre',\n",
      "       'sentiment_scores_hyp', 'sentiment_pre_labels', 'sentiment_hyp_labels',\n",
      "       'Combined_Sentiment', 'com_femotion'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title1_en</th>\n",
       "      <th>title2_en</th>\n",
       "      <th>bd_label</th>\n",
       "      <th>quora_label</th>\n",
       "      <th>Premise_Emotion_Label</th>\n",
       "      <th>Hypothesis_Emotion_Label</th>\n",
       "      <th>femotion</th>\n",
       "      <th>NV_Orc</th>\n",
       "      <th>EM_Orc</th>\n",
       "      <th>weights</th>\n",
       "      <th>test_wts</th>\n",
       "      <th>sentiment_scores_pre</th>\n",
       "      <th>sentiment_scores_hyp</th>\n",
       "      <th>sentiment_pre_labels</th>\n",
       "      <th>sentiment_hyp_labels</th>\n",
       "      <th>Combined_Sentiment</th>\n",
       "      <th>com_femotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>321198</td>\n",
       "      <td>wanda collapses: china 's richest man wang jia...</td>\n",
       "      <td>news: the detention of wang jianlin and his fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544454</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.090542</td>\n",
       "      <td>0.027466</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321201</td>\n",
       "      <td>wanda collapses: china 's richest man wang jia...</td>\n",
       "      <td>wang jian 's family is detained</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544454</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.090542</td>\n",
       "      <td>0.218851</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>321200</td>\n",
       "      <td>wanda collapses: china 's richest man wang jia...</td>\n",
       "      <td>Exclusive news: Wang Jian-lin is detained in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544454</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.090542</td>\n",
       "      <td>0.097115</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>321202</td>\n",
       "      <td>wanda collapses: china 's richest man wang jia...</td>\n",
       "      <td>prc: wang jianlin and his family are detained ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544454</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.090542</td>\n",
       "      <td>0.131030</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>321205</td>\n",
       "      <td>The land price 130 thousand, now only needs 33...</td>\n",
       "      <td>$160,000 on the floor. Now it's only $100,000....</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.544454</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>-0.024370</td>\n",
       "      <td>0.045427</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  ... com_femotion\n",
       "0  321198  ...            1\n",
       "1  321201  ...            1\n",
       "2  321200  ...            1\n",
       "3  321202  ...            1\n",
       "4  321205  ...            1\n",
       "\n",
       "[5 rows x 18 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#################### Importing ByteDance Datasets ####################\n",
    "# Train set\n",
    "train_df = pd.read_csv('train_bd_mt.csv')\n",
    "print(train_df.columns)\n",
    "le = LabelEncoder()\n",
    "train_df['bd_label'] = le.fit_transform(train_df['bd_label'])\n",
    "train_df.head()\n",
    "\n",
    "# Test set\n",
    "test_df = pd.read_csv('test_bd_mt.csv')\n",
    "print(test_df.columns)\n",
    "test_df['bd_label'] = le.transform(test_df['bd_label'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l44HsvesycNl",
    "outputId": "294e97b0-b44d-4b53-eea5-6b2a604f5ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (101239, 768)\n",
      "Hypothesis (101239, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_bd = np.load(\"pre_bert_bd.npy\")\n",
    "hyp_bert_bd = np.load(\"hyp_bert_bd.npy\")\n",
    "print('Premise', pre_bert_bd.shape)\n",
    "print('Hypothesis', hyp_bert_bd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdzlrksX5ObH",
    "outputId": "a3bb50ee-640e-4f5e-8514-7af95a4e9635"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise (28746, 768)\n",
      "Hypothesis (28746, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_bert_bd_test = np.load(\"pre_bert_test_bd.npy\")\n",
    "hyp_bert_bd_test = np.load(\"hyp_bert_test_bd.npy\")\n",
    "print('Premise', pre_bert_bd_test.shape)\n",
    "print('Hypothesis', hyp_bert_bd_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ES43kK9B2VI4",
    "outputId": "5e1f1a43-54e4-408e-b601-b4dc0be9a5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101239\n",
      "101239\n",
      "31214\n",
      "50415\n",
      "Train Length is 81629\n",
      "Test merged 25300\n",
      "Dataset length is 106929\n"
     ]
    }
   ],
   "source": [
    "train_lst_1 = train_df['title1_en'].tolist()\n",
    "print(len(train_lst_1))\n",
    "train_lst_1[:5]\n",
    "train_lst_2 = train_df['title2_en'].tolist()\n",
    "print(len(train_lst_2))\n",
    "uq_tr_1 = list(set(train_lst_1))\n",
    "uq_tr_2 = list(set(train_lst_2))\n",
    "print(len(uq_tr_1))\n",
    "print(len(uq_tr_2))\n",
    "train_merged = uq_tr_1 + uq_tr_2\n",
    "print('Train Length is', len(train_merged))\n",
    "train_merged[:5]\n",
    "test_lst_1 = test_df['title1_en'].tolist()\n",
    "test_lst_2 = test_df['title2_en'].tolist()\n",
    "uq_ts_1 = list(set(test_lst_1))\n",
    "uq_ts_2 = list(set(test_lst_2))\n",
    "test_merged = uq_ts_1 + uq_ts_2\n",
    "print('Test merged', len(test_merged))\n",
    "total_dataset = train_merged + test_merged\n",
    "print('Dataset length is', len(total_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsX3TmjS2Xd7"
   },
   "outputs": [],
   "source": [
    "# Defining the tokenizer\n",
    "def get_tokenizer(vocabulary_size):\n",
    "  print('Training tokenizer...')\n",
    "  tokenizer = Tokenizer(num_words= vocabulary_size)\n",
    "  tweet_text = []\n",
    "  print('Read {} Sentences'.format(len(total_dataset)))\n",
    "  tokenizer.fit_on_texts(total_dataset)\n",
    "  return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCAXo9332ZS4"
   },
   "outputs": [],
   "source": [
    "# For getting the embedding matrix\n",
    "def get_embeddings():\n",
    "  print('Generating embeddings matrix...')\n",
    "  embeddings_file = 'glove.6B.300d.txt'\n",
    "  embeddings_index = dict()\n",
    "  with open(embeddings_file, 'r', encoding=\"utf-8\") as infile:\n",
    "    for line in infile:\n",
    "      values = line.split()\n",
    "      word = values[0]\n",
    "      vector = np.asarray(values[1:], \"float32\")\n",
    "      embeddings_index[word] = vector\n",
    "\t# create a weight matrix for words in training docs\n",
    "  vocabulary_size = len(embeddings_index)\n",
    "  embeddinds_size = list(embeddings_index.values())[0].shape[0]\n",
    "  print('Vocabulary = {}, embeddings = {}'.format(vocabulary_size, embeddinds_size))\n",
    "  tokenizer = get_tokenizer(vocabulary_size)\n",
    "  embedding_matrix = np.zeros((vocabulary_size, embeddinds_size))\n",
    "  considered = 0\n",
    "  total = len(tokenizer.word_index.items())\n",
    "  for word, index in tokenizer.word_index.items():\n",
    "    if index > vocabulary_size - 1:\n",
    "      print(word, index)\n",
    "      continue\n",
    "    else:\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        considered += 1\n",
    "  print('Considered ', considered, 'Left ', total - considered)\t\t\t\n",
    "  return embedding_matrix, tokenizer, embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x0tkqodF2bPx"
   },
   "outputs": [],
   "source": [
    "def get_data(tokenizer, MAX_LENGTH, input_df):\n",
    "  print('Loading data')\n",
    "  X1, X2, Y = [], [], []\n",
    "\t# with open(input_file) as infile:\n",
    "\t# \tfor line in infile:\n",
    "\t# \t\tdata = line.split(',')\n",
    "\t# \t\ttext, annotation = data[2], data[1]\n",
    "\t\t\t\n",
    "\t# \t\tif annotation == \"MET\":\n",
    "\t# \t\t\tX.append(text)\n",
    "\t# \t\t\tY.append(\"1\")\n",
    "\t# \t\telif annotation == \"Non_MET\" or annotation == \"Help\":\t\n",
    "\t# \t\t\tX.append(text)\n",
    "\t# \t\t\tY.append(\"0\")\n",
    "  X1 = input_df['title1_en'].tolist()\n",
    "  X2 = input_df['title2_en'].tolist()\n",
    "  Y_nv = input_df['quora_label'].tolist()\n",
    "  Y_em = input_df['com_femotion'].tolist()\n",
    "  Y_bd = input_df['bd_label'].tolist()\n",
    "  Y_st = input_df['Combined_Sentiment'].tolist()\n",
    "  \n",
    "  assert len(X1) == len(X2) == len(Y_nv) == len(Y_em) == len(Y_bd)\n",
    "  sequences_1 = tokenizer.texts_to_sequences(X1)\n",
    "  sequences_2 = tokenizer.texts_to_sequences(X2)\n",
    "\t# for i, s in enumerate(sequences):\n",
    "\t# \tsequences[i] = sequences[i][-250:]\n",
    "  X1 = pad_sequences(sequences_1, maxlen=MAX_LENGTH)\n",
    "  X2 = pad_sequences(sequences_2, maxlen=MAX_LENGTH)\n",
    "  Y_nv = np.array(Y_nv)\n",
    "  Y_em = np.array(Y_em)\n",
    "  Y_bd = np.array(Y_bd)\n",
    "  Y_st = np.array(Y_st)\n",
    "  return X1, X2, Y_nv, Y_em, Y_st, Y_bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rlIta3aJ2eJv",
    "outputId": "8ee3317d-8f06-43d7-bb9e-48e896c56af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings matrix...\n",
      "Vocabulary = 400000, embeddings = 300\n",
      "Training tokenizer...\n",
      "Read 106929 Sentences\n",
      "Considered  22529 Left  6409\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix, tokenizer, embeddings_index = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ds_voHWS2jlH",
    "outputId": "effe2a2e-16bf-4747-d34a-b1b37948b977"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 20\n",
    "# read ml data\n",
    "X1, X2, Y_nv, Y_em, Y_st, Y_bd = get_data(tokenizer, MAX_LENGTH, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5yfwOOPf5ohr",
    "outputId": "d5b40fdc-f5da-4303-bc05-4335ea08b1c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    }
   ],
   "source": [
    "X1_test, X2_test, Y_nv_test, Y_em_test, Y_st_test, Y_bd_test = get_data(tokenizer, MAX_LENGTH, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-NKQgQYNaFf"
   },
   "outputs": [],
   "source": [
    "# Scaffold labels\n",
    "novel = embeddings_index['original']\n",
    "duplicate = embeddings_index['duplicate']\n",
    "emotion_true = embeddings_index['anticipation']+embeddings_index['sadness']+embeddings_index['joy']+embeddings_index['trust']\n",
    "emotion_false = embeddings_index['anger']+embeddings_index['fear']+embeddings_index['disgust']+embeddings_index['surprise']\n",
    "sentiment_pos = embeddings_index['positive']\n",
    "sentiment_neg = embeddings_index['negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pLtVAyHrNbMm",
    "outputId": "b088b026-3224-404a-852f-2e3323ee417a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (101239, 300)\n",
      "Test bias (28746, 300)\n"
     ]
    }
   ],
   "source": [
    "# Novelty Bias\n",
    "train_bias_nv = []\n",
    "test_bias_nv = []\n",
    "for i, row in train_df.iterrows():\n",
    "    if row['quora_label'] == 0:\n",
    "        train_bias_nv.append(novel)\n",
    "    elif row['quora_label'] == 1:\n",
    "        train_bias_nv.append(duplicate)\n",
    "    else:\n",
    "        print('Error in Train please check')\n",
    "for i, row in test_df.iterrows():\n",
    "    if row['quora_label'] == 0:\n",
    "        test_bias_nv.append(novel)\n",
    "    elif row['quora_label'] == 1:\n",
    "        test_bias_nv.append(duplicate)\n",
    "    else:\n",
    "        print('Error in Test please check')\n",
    "train_bias_nv = np.stack(train_bias_nv)\n",
    "test_bias_nv = np.stack(test_bias_nv)\n",
    "print('Train bias', train_bias_nv.shape)\n",
    "print('Test bias', test_bias_nv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hsa3KHTdOKs9",
    "outputId": "dd71525c-327c-4d33-a293-890f0d61abeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (101239, 300)\n",
      "Test bias (28746, 300)\n"
     ]
    }
   ],
   "source": [
    "# Emotion Bias\n",
    "train_bias_em = []\n",
    "test_bias_em = []\n",
    "zero_vector = np.zeros((300,))\n",
    "for i in range(len(train_df)):\n",
    "    # pre = train_df.loc[i, 'Premise_Emotion_Label']\n",
    "    # hyp = train_df.loc[i, 'Hypothesis_Emotion_Label']\n",
    "    em_lab = train_df.loc[i, 'com_femotion']\n",
    "    if train_df.loc[i, 'bd_label'] == 0 and em_lab == 1:\n",
    "        train_bias_em.append(emotion_false)\n",
    "    elif train_df.loc[i, 'bd_label'] == 1 and em_lab == 0:\n",
    "        train_bias_em.append(emotion_true)\n",
    "    else:\n",
    "        train_bias_em.append(zero_vector)\n",
    "for i in range(len(test_df)):\n",
    "    # pre = test_df.loc[i, 'Premise_Emotion_Label']\n",
    "    # hyp = test_df.loc[i, 'Hypothesis_Emotion_Label']\n",
    "    em_lab = test_df.loc[i, 'com_femotion']\n",
    "    if test_df.loc[i, 'bd_label'] == 0 and em_lab == 1:\n",
    "        test_bias_em.append(emotion_false)\n",
    "    elif test_df.loc[i, 'bd_label'] == 1 and em_lab == 0:\n",
    "        test_bias_em.append(emotion_true)\n",
    "    else:\n",
    "        test_bias_em.append(zero_vector)\n",
    "train_bias_em = np.stack(train_bias_em)\n",
    "test_bias_em = np.stack(test_bias_em)\n",
    "print('Train bias', train_bias_em.shape)\n",
    "print('Test bias', test_bias_em.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zP8B8J4PGvLM",
    "outputId": "3861245a-75bb-40b5-efde-4f74b1eeaaee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train bias (101239, 300)\n",
      "Test bias (28746, 300)\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Bias\n",
    "train_bias_st = []\n",
    "test_bias_st = []\n",
    "zero_vector = np.zeros((300,))\n",
    "for i in range(len(train_df)):\n",
    "    hyp = train_df.loc[i, 'Combined_Sentiment']\n",
    "    if hyp == 0 and train_df.loc[i, 'bd_label'] == 0:\n",
    "        train_bias_st.append(sentiment_neg)\n",
    "    # elif hyp == 0 and train_df.loc[i, 'stance'] == 2:\n",
    "    #     train_bias_em.append(emotion_true)\n",
    "    elif hyp == 1 and train_df.loc[i, 'bd_label'] == 1:\n",
    "        train_bias_st.append(sentiment_pos)\n",
    "    else:\n",
    "        #print('in here')\n",
    "        train_bias_st.append(zero_vector)\n",
    "for i in range(len(test_df)):\n",
    "    hyp = test_df.loc[i, 'Combined_Sentiment']\n",
    "    if hyp == 0 and test_df.loc[i, 'bd_label'] == 0:\n",
    "        test_bias_st.append(sentiment_neg)\n",
    "    # elif hyp == 0 and test_df.loc[i, 'stance'] == 2:\n",
    "    #     test_bias_em.append(emotion_true)\n",
    "    elif hyp == 1 and test_df.loc[i, 'bd_label'] == 1:\n",
    "        test_bias_st.append(sentiment_pos)\n",
    "    else:\n",
    "        test_bias_st.append(zero_vector)\n",
    "train_bias_st = np.stack(train_bias_st)\n",
    "test_bias_st = np.stack(test_bias_st)\n",
    "print('Train bias', train_bias_st.shape)\n",
    "print('Test bias', test_bias_st.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7dfS5WyN7mO"
   },
   "outputs": [],
   "source": [
    "# Considering the final train and test bias\n",
    "train_bias = np.add(train_bias_em, train_bias_st)\n",
    "test_bias = np.add(test_bias_em, test_bias_st)\n",
    "# train_bias = np.add(train_bias_nv, train_bias_em, train_bias_st)\n",
    "# test_bias = np.add(test_bias_nv, test_bias_em, test_bias_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6Nisf7d5rFB",
    "outputId": "61857224-75bf-4c51-f3ef-6bf7b2b0b994"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Creating one-hot encodings\n",
    "y_train_nv = keras.utils.to_categorical(Y_nv)\n",
    "print(y_train_nv)\n",
    "y_train_em = keras.utils.to_categorical(Y_em)\n",
    "print(y_train_em)\n",
    "y_train_bd = keras.utils.to_categorical(Y_bd)\n",
    "print(y_train_bd)\n",
    "y_train_st = keras.utils.to_categorical(Y_st)\n",
    "print(y_train_st)\n",
    "y_test_nv = keras.utils.to_categorical(Y_nv_test)\n",
    "print(y_test_nv)\n",
    "y_test_em = keras.utils.to_categorical(Y_em_test)\n",
    "print(y_test_em)\n",
    "y_test_bd = keras.utils.to_categorical(Y_bd_test)\n",
    "print(y_test_bd)\n",
    "y_test_st = keras.utils.to_categorical(Y_st_test)\n",
    "print(y_test_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cpk4PCS5wn5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "VALIDATION_RATIO = 0.1\n",
    "RANDOM_STATE = 9527\n",
    "x1_train, x1_val, \\\n",
    "x2_train, x2_val, \\\n",
    "x1_train_bert, x1_val_bert, \\\n",
    "x2_train_bert, x2_val_bert, \\\n",
    "y_train_nv, y_val_nv, \\\n",
    "y_train_em, y_val_em, \\\n",
    "y_train_st, y_val_st, \\\n",
    "y_train_bd, y_val_bd, \\\n",
    "train_bias, val_bias, \\\n",
    "train_bias_nv, val_bias_nv, \\\n",
    "train_bias_em, val_bias_em = \\\n",
    "    train_test_split(\n",
    "        X1, X2, \n",
    "        pre_bert_bd, hyp_bert_bd,\n",
    "        y_train_nv, y_train_em, y_train_st,\n",
    "        y_train_bd, train_bias,\n",
    "        train_bias_nv, train_bias_em,\n",
    "        test_size=VALIDATION_RATIO, \n",
    "        random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rer-BbP150ex",
    "outputId": "d11cec52-a730-4ff3-f3f2-4d4837155552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "----------\n",
      "x1_train: (91115, 20)\n",
      "x2_train: (91115, 20)\n",
      "x1_train_bert : (91115, 768)\n",
      "y_train_bd : (91115, 2)\n",
      "----------\n",
      "x1_val:   (10124, 20)\n",
      "x2_val:   (10124, 20)\n",
      "x2_val:   (10124, 20)\n",
      "x1_val_bert :   (10124, 768)\n",
      "----------\n",
      "Test Set\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Set\")\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_train: {x1_train.shape}\")\n",
    "print(f\"x2_train: {x2_train.shape}\")\n",
    "print(f\"x1_train_bert : {x1_train_bert.shape}\")\n",
    "print(f\"y_train_bd : {y_train_bd.shape}\")\n",
    "\n",
    "print(\"-\" * 10)\n",
    "print(f\"x1_val:   {x1_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"x2_val:   {x2_val.shape}\")\n",
    "print(f\"x1_val_bert :   {x1_val_bert.shape}\")\n",
    "print(\"-\" * 10)\n",
    "print(\"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8kBTv8G52vx"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "\n",
    "NUM_LSTM_UNITS = 300\n",
    "\n",
    "MAX_NUM_WORDS = embedding_matrix.shape[0]\n",
    "\n",
    "NUM_EMBEDDING_DIM = embedding_matrix.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vuHhWcP_zjhn",
    "outputId": "8892fb7f-6e66-4df4-c6ac-308d3716e245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 768)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 20, 300)      120000000   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 768)       0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 1, 768)       0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 20, 300)      541200      embedding[0][0]                  \n",
      "                                                                 embedding[1][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 1, 300)       1102800     reshape[0][0]                    \n",
      "                                                                 reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 20, 600)      0           bidirectional[0][0]              \n",
      "                                                                 bidirectional[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 600)       0           bidirectional_2[0][0]            \n",
      "                                                                 bidirectional_2[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 300)          901200      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional) (None, 300)          901200      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (None, 300)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_1 (TFOpLambda) (None, 300)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, 300)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda)   (None, 300)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply (TFOpLambda)   (None, 300)          0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (None, 300)          0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.math.multiply_2 (TFOpLambda) (None, 300)          0           tf.math.multiply_1[0][0]         \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 900)          0           tf.__operators__.add[0][0]       \n",
      "                                                                 tf.math.subtract[0][0]           \n",
      "                                                                 tf.math.multiply[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 600)          0           tf.__operators__.add_2[0][0]     \n",
      "                                                                 tf.math.multiply_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "pre_nv (Dense)                  (None, 64)           57664       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pre_em (Dense)                  (None, 64)           57664       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pre_st (Dense)                  (None, 64)           57664       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pre_bd (Dense)                  (None, 64)           38464       concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_7 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "nv (Dense)                      (None, 2)            130         pre_nv[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "em (Dense)                      (None, 2)            130         pre_em[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "st (Dense)                      (None, 2)            130         pre_st[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bd (Dense)                      (None, 2)            130         pre_bd[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 123,658,376\n",
      "Trainable params: 123,658,376\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# BERT + Normal Grand Model\n",
    "\n",
    "NUM_LSTM_UNITS = 150\n",
    "\n",
    "top_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "bm_input_wd = Input(\n",
    "    shape=(MAX_SEQUENCE_LENGTH, ), \n",
    "    dtype='int32')\n",
    "\n",
    "# embedding_layer = Embedding(\n",
    "#     MAX_NUM_WORDS, NUM_EMBEDDING_DIM, weights = [embedding_matrix], trainable = True)\n",
    "embedding_layer = Embedding(\n",
    "    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\n",
    "top_embedded_wd = embedding_layer(\n",
    "    top_input_wd)\n",
    "bm_embedded_wd = embedding_layer(\n",
    "    bm_input_wd)\n",
    "\n",
    "source_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_wd = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_wd = source_lstm_wd(top_embedded_wd)\n",
    "bm_source_wd = source_lstm_wd(bm_embedded_wd)\n",
    "\n",
    "source_comb_wd = concatenate(\n",
    "    [top_source_wd, bm_source_wd],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_wd = shared_lstm_wd(source_comb_wd)   # 300D vector\n",
    "\n",
    "\n",
    "top_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bm_input_bt = Input(\n",
    "    shape=(768, ), \n",
    "    dtype='float32')\n",
    "bias_input = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "bias_input_nv = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "bias_input_em = Input(\n",
    "    shape = (300, ),\n",
    "    dtype = 'float32')\n",
    "\n",
    "\n",
    "top_embedded_bt = Reshape((1, 768, ))(top_input_bt)\n",
    "bm_embedded_bt = Reshape((1, 768, ))(bm_input_bt)\n",
    "\n",
    "source_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, return_sequences=True, recurrent_dropout = 0.3))\n",
    "shared_lstm_bt = Bidirectional(LSTM(NUM_LSTM_UNITS, activation='tanh', recurrent_dropout = 0.3))\n",
    "top_source_bt = source_lstm_bt(top_embedded_bt)\n",
    "bm_source_bt = source_lstm_bt(bm_embedded_bt)\n",
    "\n",
    "source_comb_bt = concatenate(\n",
    "    [top_source_bt, bm_source_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "lstm_ops_bt = shared_lstm_bt(source_comb_bt)  #300D vector\n",
    "\n",
    "#merged = Add()([top_output, bm_output])\n",
    "#merged_bd = Add()([lstm_ops, bias_input])\n",
    "\n",
    "# Bert and Normal Combination\n",
    "comb_features = concatenate(\n",
    "    [lstm_ops_wd+lstm_ops_bt, lstm_ops_wd-lstm_ops_bt, lstm_ops_wd*lstm_ops_bt],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "comb_features_bd = concatenate(\n",
    "    [lstm_ops_wd+lstm_ops_bt+bias_input, lstm_ops_wd*lstm_ops_bt*bias_input],\n",
    "    axis=-1\n",
    "    )\n",
    "\n",
    "pre_nv = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_nv')(comb_features)\n",
    "\n",
    "pre_em = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_em')(comb_features)\n",
    "\n",
    "pre_st = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_st')(comb_features)\n",
    "\n",
    "pre_bd = Dense(\n",
    "    units=64, \n",
    "    activation='tanh',\n",
    "    name = 'pre_bd')(comb_features_bd)\n",
    "\n",
    "dense_nv =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'nv')\n",
    "\n",
    "dense_em =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'em')\n",
    "\n",
    "dense_st =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'st')\n",
    "\n",
    "dense_bd =  Dense(\n",
    "    units=NUM_CLASSES, \n",
    "    activation='softmax',\n",
    "    name = 'bd')\n",
    "\n",
    "predictions_nv = dense_nv(pre_nv)\n",
    "predictions_em = dense_em(pre_em)\n",
    "predictions_st = dense_st(pre_st)\n",
    "predictions_bd = dense_bd(pre_bd)\n",
    "\n",
    "model = Model(\n",
    "    inputs=[top_input_wd, bm_input_wd, top_input_bt, bm_input_bt, bias_input, bias_input_nv, bias_input_em], \n",
    "    outputs=[predictions_nv, predictions_em, predictions_st, predictions_bd])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jYdNBkaE58lG"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "lr = 1e-3\n",
    "opt = Adam(lr=lr, decay=lr/50)\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'nv':'categorical_crossentropy', 'em':'categorical_crossentropy', 'st':'categorical_crossentropy', 'bd':'categorical_crossentropy'},\n",
    "    loss_weights={'nv': 0, 'em':1, 'st':1, 'bd': 1},\n",
    "    metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='multitask_bdbias.h5', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiTask BERT Model\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "stop = [EarlyStopping(monitor='val_loss', patience=0.001)]\n",
    "history = model.fit(x=[x1_train, x2_train, x1_train_bert, x2_train_bert, train_bias, train_bias_nv, train_bias_em],\n",
    "                    y=[y_train_nv, y_train_em, y_train_st, y_train_bd],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    validation_data=(\n",
    "                      [x1_val, x2_val, x1_val_bert, x2_val_bert, val_bias, val_bias_nv, val_bias_em], \n",
    "                      [y_val_nv, y_val_em, y_val_st, y_val_bd]\n",
    "                    ),\n",
    "                    shuffle=True,\n",
    "                    callbacks=[stop, checkpointer],\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "car5k4Vu8sBb"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(\n",
    "    [X1_test, X2_test, pre_bert_bd_test, hyp_bert_bd_test, test_bias, test_bias_nv, test_bias_em])\n",
    "# print('Accuracy is')\n",
    "# print(metrics.accuracy_score(y_test, y_pred, sample_weight = reduced_test_weights)*100)\n",
    "# print(classification_report(y_test, y_pred, target_names = ['agreed', 'disagreed'], sample_weight = reduced_test_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ggMSo4ID8t_R",
    "outputId": "fffa9c3b-e02e-4415-badb-8b1cbc54b289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 28746, 2)\n",
      "(28746, 2)\n",
      "(28746, 2)\n",
      "(28746, 2)\n",
      "(28746, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.stack(predictions).shape)\n",
    "print(predictions[0].shape)\n",
    "print(predictions[1].shape)\n",
    "print(predictions[2].shape)\n",
    "print(predictions[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjAs8826b5x8"
   },
   "outputs": [],
   "source": [
    "# Result Labels\n",
    "res_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[3], axis=1)]\n",
    "res_df['Fake_News_Labels'] = y_pred\n",
    "#print(y_pred)\n",
    "print('BD Accuracy is')\n",
    "print(metrics.accuracy_score(Y_bd_test, y_pred, sample_weight = test_df['test_wts'].values)*100)\n",
    "print(classification_report(Y_bd_test, y_pred, target_names = ['agreed', 'disagreed'], sample_weight = test_df['test_wts'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[2], axis=1)]\n",
    "res_df['Sentiment_Labels'] = y_pred\n",
    "print('Sentiment Accuracy is')\n",
    "print(metrics.accuracy_score(Y_st_test, y_pred)*100)\n",
    "print(classification_report(Y_st_test, y_pred, target_names = ['same', 'different']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[1], axis=1)]\n",
    "res_df['Emotion_Labels'] = y_pred\n",
    "#print(y_pred)\n",
    "print('Emotion Accuracy is')\n",
    "print(metrics.accuracy_score(Y_em_test, y_pred)*100)\n",
    "print(classification_report(Y_em_test, y_pred, target_names = ['true', 'false']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [idx for idx in np.argmax(predictions[0], axis=1)]\n",
    "res_df['Novelty_Labels'] = y_pred\n",
    "#print(y_pred)\n",
    "print('NV Accuracy is')\n",
    "print(metrics.accuracy_score(Y_nv_test, y_pred)*100)\n",
    "print(classification_report(Y_nv_test, y_pred, target_names = ['novel', 'duplicate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXYrz4mtcaHJ"
   },
   "outputs": [],
   "source": [
    "# Saving the labels\n",
    "res_df.to_csv(\"BD_MtaskRes.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment_BD_Multitask.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
